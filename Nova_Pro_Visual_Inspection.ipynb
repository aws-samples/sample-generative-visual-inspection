{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Zero-Training Visual Defect Detection Using Amazon Nova Pro\n",
    "\n",
    "In manufacturing, visual quality inspection is critical for ensuring product reliability<br/>\n",
    "and compliance. However, traditional approaches—whether manual review or custom-trained<br/>\n",
    "computer vision models—are costly, slow to adapt, and difficult to scale across diverse<br/>\n",
    "product lines.\n",
    "\n",
    "This Jupyter notebook introduces a zero-training, no-dataset-required visual inspection<br/>\n",
    "system using **Amazon Nova Pro**, a multimodal foundation model accessed via **Amazon Bedrock**.<br/>\n",
    "Using only a **Jupyter notebook**, you can detect manufacturing defects in product images with structured<br/>\n",
    "natural language prompts—no computer vision expertise or labeled data required.<br/>\n",
    "By the end of this notebook, you'll be able to:\n",
    "\n",
    "* Upload and analyze product images in a Jupyter notebook\n",
    "* Detect visual defects using Amazon Nova Pro\n",
    "* Automatically return bounding boxes, failure reasons, and QC status\n",
    "* Visualize defect overlays on product images\n",
    "\n",
    "For more background consult the Readme in the same repository."
   ],
   "id": "cd43e7b7640b12d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pipeline Architecture in this Notebook\n",
    "\n",
    "This inspection pipeline operates entirely within a local Jupyter notebook and AWS serverless infrastructure:\n",
    "\n",
    "1. Image Capture: Use widgets in the notebook to upload a product image (and optionally a reference image).\n",
    "2. Image Preprocessing: Images are resized, converted to Base64, and prepared for inference.\n",
    "3. Generative AI Inference: Amazon Bedrock invokes Nova Pro to analyze the image and return structured defect data.\n",
    "4. Visualization: Bounding boxes and defect reasons are drawn using matplotlib.\n",
    "\n",
    "User → Jupyter Notebook → Amazon Bedrock (Nova Pro) → JSON Defect Output → Matplotlib Overlay\n",
    "\n",
    "## Step-by-Step Implementation\n",
    "\n",
    "The Jupyter notebook requires only an environment with internet access and AWS credentials to access Bedrock.<br/>\n",
    "So you can run this locally on your laptop or on an **Amazon Sagemaker AI** notebook.\n",
    "\n",
    "Install and import the required libraries. We need boto3 to invoke *Amazon Nova Pro* via  Amazon Bedrock, pillow <br/>\n",
    "is Python Image Library and matplotlib to draw the detected areas on the images."
   ],
   "id": "57ce0dd5b00c06d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install boto3 pillow matplotlib --quiet",
   "id": "23405b29-4f77-411c-89fe-65c69a278fbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe3653-02eb-42c0-98e7-bac973e3c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from textwrap import dedent"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Define the model (currently we only tested **Amazon Nova** in the *light* and *pro* variant) and the endpoint.<br/>\n",
    "Ensure beforehand that you have access to the model in the specified AWS region."
   ],
   "id": "a16cf8c8f9610547"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573e82b-ab15-440a-a8b6-ca58d230b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "MODEL_ID = \"amazon.nova-pro-v1:0\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For the first test we will reference two images. The `ref` image is the reference image and the `qc` image is the image to test of the model is detecting anomalies on that image.",
   "id": "2cfc2f02b55dadb4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc339a67-13e7-4fdb-aff2-bb2be0fc35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Replace these with your file paths ====\n",
    "qc_image_path = \"./sample-dataset/metalplate_nok.jpg\"\n",
    "ref_image_path = \"./sample-dataset/metalplate_reference.jpg\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define helper functions to be able to access images, encode them as well as resize and upload the files.",
   "id": "8290f67ea44a3dca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd1c35-28a4-4086-a4c0-57c6065ff9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def get_png_base64_from_file(filepath, max_size=(1024, 1024)):\n",
    "    img = Image.open(filepath)\n",
    "    img.thumbnail(max_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    with io.BytesIO() as output:\n",
    "        img.save(output, format=\"PNG\")\n",
    "        png_bytes = output.getvalue()\n",
    "        base64_str = base64.b64encode(png_bytes).decode(\"utf-8\")\n",
    "\n",
    "    return base64_str, png_bytes, img\n",
    "\n",
    "# Load and display QC image\n",
    "def load_qc_image():\n",
    "    global base64_image, image_data, img\n",
    "    b64, raw, preview = get_png_base64_from_file(qc_image_path)\n",
    "    base64_image, image_data, img = b64, raw, preview\n",
    "    with qc_display:\n",
    "        clear_output(wait=True)\n",
    "        w, h = img.size\n",
    "        img_rez = img.resize((w // 3, h // 3))\n",
    "        display(img_rez)\n",
    "\n",
    "# Load and display Reference image\n",
    "def load_ref_image():\n",
    "    global base64_reference_image, reference_image_data, ref_img\n",
    "    b64, raw, preview = get_png_base64_from_file(ref_image_path)\n",
    "    base64_reference_image, reference_image_data, ref_img = b64, raw, preview\n",
    "    with ref_display:\n",
    "        clear_output(wait=True)\n",
    "        w, h = ref_img.size\n",
    "        ref_img_rez = ref_img.resize((w // 3, h // 3))\n",
    "        display(ref_img_rez)\n",
    "\n",
    "qc_display = widgets.Output()\n",
    "ref_display = widgets.Output()\n",
    "display(widgets.VBox([\n",
    "    widgets.Label(\"🧾 Reference Image:\"), ref_display,\n",
    "    widgets.Label(\"📸 QC Image:\"), qc_display\n",
    "]))\n",
    "\n",
    "# Load both images\n",
    "load_qc_image()\n",
    "load_ref_image()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Define the system prompt. As you see it defines the special awareness so we can calculate bounding<br/>boxes to the images after the model has run.\n",
    "\n",
    "You also see that we request a specific JSON structure to automatically process the output of the model.\n",
    "\n",
    "Then there function for\n",
    "* building the request out of the prompt and the images\n",
    "* sending the request to Bedrock\n",
    "* parsing and cleaning the response (cleaning in case there is markdown included so we can automatically process the response)"
   ],
   "id": "1e48e765ee4878d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86205876-d2b5-4808-89c7-585490f02ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "default_system_prompt = dedent(\"\"\"\\\n",
    "    Compare the current image with the reference image \n",
    "    and highlight any visual differences. Mark as NOK if differences exist; otherwise, mark as OK. \n",
    "    Be very strict with the differences, give me all the differences. Check for any missing or visual differences.\n",
    "    Scratches and dents are also important. Search for any aestethical problem, any barcode missing, if the objects have some \n",
    "    missing labels, or anything aestethically different, blurry text or logos. Any material differences, scratches on surface,\n",
    "    color difference between reference and QC objects\n",
    "\"\"\")\n",
    "\n",
    "# Prompt that requests a structured JSON\n",
    "instruction = dedent(\"\"\"\\\n",
    "    Provide me the JSON with the written description in 'text' and the list of objects in 'objects'.\n",
    "    This list must include: name, color, qc, reason, bounding_box of the defect (x_min, y_min, x_max, y_max).\n",
    "    Clean JSON only — no markdown, no extra characters.\n",
    "    If you describe a defect, include its bounding box.\n",
    "    Do not group bounding boxes or include any from the reference image. Be strict on the JSON based on the example,\n",
    "    keep the same names for the objects because I am parsing those later. Make bounding boxes for all defects.\n",
    "    Example JSON:\n",
    "    {\n",
    "        \"text\": \"The object is a blue and green sponge. The green part has white spots...\",\n",
    "        \"objects\": [\n",
    "            {\n",
    "                \"name\": \"defect_1\",\n",
    "                \"color\": \"green\",\n",
    "                \"qc\": \"NOK\",\n",
    "                \"reason\": \"white spots\",\n",
    "                \"bounding_box\": {\n",
    "                    \"x_min\": 195, \"y_min\": 475, \"x_max\": 285, \"y_max\": 595\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"defect_2\",\n",
    "                \"color\": \"blue\",\n",
    "                \"qc\": \"NOK\",\n",
    "                \"reason\": \"white spot and small hole\",\n",
    "                \"bounding_box\": {\n",
    "                    \"x_min\": 690, \"y_min\": 420, \"x_max\": 760, \"y_max\": 500\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "def build_qc_request(image_data, instruction, default_system_prompt, reference_image_data=None):\n",
    "    system_list = [{\"text\": default_system_prompt}]\n",
    "    \n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"image\": {\"format\": \"png\", \"source\": {\"bytes\": base64.b64encode(image_data).decode()}}},\n",
    "                {\"text\": instruction}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if reference_image_data:\n",
    "        message_list.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"text\": \"This is the reference image. Do not include bounding boxes for this image.\"},\n",
    "                {\"image\": {\"format\": \"png\", \"source\": {\"bytes\": base64.b64encode(reference_image_data).decode()}}}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"schemaVersion\": \"messages-v1\",\n",
    "        \"messages\": message_list,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": {\n",
    "            \"max_new_tokens\": 2500,\n",
    "            \"top_p\": 0.1,\n",
    "            \"top_k\": 20,\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "    }\n",
    "\n",
    "def invoke_qc_model(native_request, model_id):\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(modelId=model_id, body=json.dumps(native_request))\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        print(\"✅ Inference completed successfully.\")\n",
    "        return model_response\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model invocation error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def parse_qc_response(model_response):\n",
    "    if not model_response:\n",
    "        return None\n",
    "    try:\n",
    "        raw_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        clean_json = raw_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        parsed = json.loads(clean_json)\n",
    "        print(\"✅ Parsed QC Output:\")\n",
    "        print(json.dumps(parsed, indent=2))\n",
    "        return parsed\n",
    "    except (KeyError, json.JSONDecodeError) as e:\n",
    "        print(f\"❌ Response parsing failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "native_request = build_qc_request(image_data, instruction, default_system_prompt, reference_image_data)\n",
    "model_response = invoke_qc_model(native_request, MODEL_ID)\n",
    "parsed_output = parse_qc_response(model_response)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have the response we need to draw the bounding boxes of the detected faults over the images.\n",
    "Matplotlib helps to do this.\n",
    "\n",
    "Afterwards we show the reference image, the resuling image with drawing bouning boxes if there were detected errors <br/>\n",
    "and the result description given by the model."
   ],
   "id": "7a9863a7ad218f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ac4c7-d73f-4ad6-a11b-28cbe7f01b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(base64_img, objects):\n",
    "    img = Image.open(io.BytesIO(base64.b64decode(base64_img)))\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    model_width = 1000\n",
    "    model_height = 800\n",
    "\n",
    "    scale_x = img_width / model_width\n",
    "    scale_y = img_height / model_height\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img)\n",
    "\n",
    "    for obj in objects:\n",
    "        box = obj[\"bounding_box\"]\n",
    "        x_min = box[\"x_min\"] * scale_x\n",
    "        y_min = box[\"y_min\"] * scale_y\n",
    "        x_max = box[\"x_max\"] * scale_x\n",
    "        y_max = box[\"y_max\"] * scale_y\n",
    "        label = f\"{obj['name']} ({obj['qc']})\"\n",
    "        color = \"green\" if obj[\"qc\"] == \"OK\" else \"red\"\n",
    "\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                             linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min - 10, label, color=color, fontsize=12, weight='bold')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def display_qc_report(parsed, base64_image, base64_reference_image=None):\n",
    "    if base64_reference_image:\n",
    "        ref_img = Image.open(io.BytesIO(base64.b64decode(base64_reference_image)))\n",
    "        w, h = ref_img.size\n",
    "        resized_ref = ref_img.resize((w // 2, h // 2))\n",
    "        print(\"🧾 Reference Image:\")\n",
    "        display(resized_ref)\n",
    "\n",
    "    if parsed.get(\"objects\"):\n",
    "        print(\"🧾 QC Image with Defects:\")\n",
    "        draw_bounding_boxes(base64_image, parsed[\"objects\"])\n",
    "\n",
    "        report_text = f\"\"\"QC: {parsed['objects'][0]['qc']}\n",
    "\n",
    "Description:\n",
    "{parsed['text']}\n",
    "\n",
    "Defects:\"\"\"\n",
    "\n",
    "        for obj in parsed[\"objects\"]:\n",
    "            box = obj[\"bounding_box\"]\n",
    "            report_text += f\"\"\"\n",
    "- {obj['name']} ({obj['color']}): {obj['qc']} — reason: {obj['reason']}\n",
    "  Bounding Box: x_min={box['x_min']}, y_min={box['y_min']}, x_max={box['x_max']}, y_max={box['y_max']}\n",
    "\"\"\"\n",
    "    else:\n",
    "        qc_img = Image.open(io.BytesIO(base64.b64decode(base64_image)))\n",
    "        w, h = qc_img.size\n",
    "        resized_qc = qc_img.resize((w // 2, h // 2))\n",
    "        print(\"🧾 QC Image:\")\n",
    "        display(resized_qc)\n",
    "\n",
    "        report_text = f\"\"\"QC: OK\n",
    "\n",
    "Description:\n",
    "{parsed['text']}\n",
    "\n",
    "No defects were detected.\n",
    "\"\"\"\n",
    "\n",
    "    print(\"\\n🧾 Quality Control Report:\")\n",
    "    print(report_text)\n",
    "    \n",
    "display_qc_report(parsed_output, base64_image, base64_reference_image)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Example\n",
    "\n",
    "So lets use some different images. To process those and further images lets alos define a function that call the whole pipline."
   ],
   "id": "487f9cd94b3653d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8dad5b-9936-413d-9c0a-ddfb9164a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_image_path = \"./sample-dataset/metalplate_nok2.jpg\"\n",
    "ref_image_path = \"./sample-dataset/metalplate_reference.jpg\"\n",
    "\n",
    "def run_qc_full_pipeline(qc_image_path, ref_image_path):\n",
    "    load_qc_image()\n",
    "    load_ref_image()\n",
    "    native_request = build_qc_request(image_data, instruction, default_system_prompt, reference_image_data)\n",
    "    model_response = invoke_qc_model(native_request, MODEL_ID)\n",
    "    parsed_output = parse_qc_response(model_response)\n",
    "    display_qc_report(parsed_output, base64_image, base64_reference_image)\n",
    "\n",
    "run_qc_full_pipeline(qc_image_path, ref_image_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Example",
   "id": "fe2b8f5c4cc7f1fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fb20a-200e-4e39-bb3e-fd5529cfb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_image_path = \"./sample-dataset/metalpiece_nok.jpg\"\n",
    "ref_image_path = \"./sample-dataset/metalpiece_reference.jpg\"\n",
    "run_qc_full_pipeline(qc_image_path, ref_image_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Example",
   "id": "e25f792c40fd1ea3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e66cd-5b5c-4e8f-904e-b92408e55345",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_image_path = \"./sample-dataset/plasticpart_nok.jpg\"\n",
    "ref_image_path = \"./sample-dataset/plasticpart_reference.jpg\"\n",
    "run_qc_full_pipeline(qc_image_path, ref_image_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Example",
   "id": "baac6025c9bea64b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b8025-da1c-4c38-8b9c-8c671e972759",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_image_path = \"./sample-dataset/cubesponge2_nok.jpg\"\n",
    "ref_image_path = \"./sample-dataset/cubesponge2_reference.jpg\"\n",
    "run_qc_full_pipeline(qc_image_path, ref_image_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Example",
   "id": "4680e41694985b68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8344bc-3d63-4323-ad52-73aeca765f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_image_path = \"./sample-dataset/cubesponge2_nok2.jpg\"\n",
    "ref_image_path = \"./sample-dataset/cubesponge2_reference.jpg\"\n",
    "run_qc_full_pipeline(qc_image_path, ref_image_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Example",
   "id": "666f33c693d1a207"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3fc38-fb54-4dc3-9b86-30af1799d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_image_path = \"./sample-dataset/cubesponge_nok.jpg\"\n",
    "ref_image_path = \"./sample-dataset/cubesponge_reference.jpg\"\n",
    "run_qc_full_pipeline(qc_image_path, ref_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
