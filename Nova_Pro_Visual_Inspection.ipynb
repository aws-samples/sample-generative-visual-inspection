{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Zero-Training Visual Defect Detection Using Amazon Nova Pro\n",
    "\n",
    "In manufacturing, visual quality inspection is critical for ensuring product reliability<br/>\n",
    "and compliance. However, traditional approaches—whether manual review or custom-trained<br/>\n",
    "computer vision models—are costly, slow to adapt, and difficult to scale across diverse<br/>\n",
    "product lines.\n",
    "\n",
    "This Jypiter notebook introduces a zero-training, no-dataset-required visual inspection<br/>\n",
    "system using **Amazon Nova Pro**, a multimodal foundation model accessed via **Amazon Bedrock**.<br/>\n",
    "Using only a **Jupyter notebook**, you can detect manufacturing defects in product images with structured<br/>\n",
    "natural language prompts—no computer vision expertise or labeled data required.<br/>\n",
    "By the end of this notebook, you'll be able to:\n",
    "\n",
    "* Upload and analyze product images in a Jupyter notebook\n",
    "* Detect visual defects using Amazon Nova Pro\n",
    "* Automatically return bounding boxes, failure reasons, and QC status\n",
    "* Visualize defect overlays on product images\n",
    "\n",
    "For more background consult the Readme in the same repository."
   ],
   "id": "10b62d62b689fc02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pipeline Architecture in this Notebook\n",
    "\n",
    "This inspection pipeline operates entirely within a local Jupyter notebook and AWS serverless infrastructure:\n",
    "\n",
    "1. Image Capture: Use widgets in the notebook to upload a product image (and optionally a reference image).\n",
    "2. Image Preprocessing: Images are resized, converted to Base64, and prepared for inference.\n",
    "3. AI Inference: Amazon Bedrock invokes Nova Pro to analyze the image and return structured defect data.\n",
    "4. Visualization: Bounding boxes and defect reasons are drawn using matplotlib.\n",
    "\n",
    "User → Jupyter Notebook → Amazon Bedrock (Nova Pro) → JSON Defect Output → Matplotlib Overlay\n",
    "\n",
    "## Step-by-Step Implementation\n",
    "\n",
    "The Jupyter notebook requires only an environment with internet access and AWS credentials to access Bedrock.<br/>\n",
    "So you can run this locally on your laptop or on an **Amazon Sagemaker AI** notebook.\n",
    "\n",
    "Install and import the required libraries. We need boto3 to invoke *Amazon Nova Pro* via  Amazon Bedrock, pillow <br/>\n",
    "is Python Image Library and matplotlib to draw the detected areas on the images."
   ],
   "id": "c0d40fe6bda101f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T06:41:46.977566Z",
     "start_time": "2025-04-29T06:41:46.855075Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": "!pip install boto3 pillow matplotlib --quiet",
   "id": "23405b29-4f77-411c-89fe-65c69a278fbc"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aabe3653-02eb-42c0-98e7-bac973e3c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from textwrap import dedent"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Define the model (currently we only tested **Amazon Nova** in the *light* and *pro* variant) and the endpoint.<br/>\n",
    "Ensure beforehand that you have access to the model in the specified AWS region."
   ],
   "id": "f467fe1e4cc1b509"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f573e82b-ab15-440a-a8b6-ca58d230b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "MODEL_ID = \"amazon.nova-pro-v1:0\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define helper functions to be able to access images, encode them as well as resize and upload the files.",
   "id": "7bbb5871d5f4c264"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33cd1c35-28a4-4086-a4c0-57c6065ff9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d7ef65499f4616a2994d9f51e4b98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='📸 Upload QC Image:'), FileUpload(value=(), accept='image/*', description='Upload')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Upload widgets\n",
    "qc_uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "ref_uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "\n",
    "# Output areas for images\n",
    "qc_display = widgets.Output()\n",
    "ref_display = widgets.Output()\n",
    "\n",
    "# Show the widgets\n",
    "display(widgets.VBox([\n",
    "    widgets.Label(\"📸 Upload QC Image:\"), qc_uploader, qc_display,\n",
    "    widgets.Label(\"🧾 Upload Reference Image (optional):\"), ref_uploader, ref_display\n",
    "]))\n",
    "\n",
    "# PNG conversion + resize to original size\n",
    "def get_png_base64_from_bytes(original_bytes, max_size=(1024, 1024), scale=0.3):\n",
    "    img = Image.open(io.BytesIO(original_bytes))\n",
    "    img.thumbnail(max_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Resize to scale %\n",
    "    width, height = img.size\n",
    "    resized_img = img.resize((int(width * scale), int(height * scale)))\n",
    "\n",
    "    with io.BytesIO() as output:\n",
    "        img.save(output, format=\"PNG\")\n",
    "        png_bytes = output.getvalue()\n",
    "        base64_str = base64.b64encode(png_bytes).decode(\"utf-8\")\n",
    "\n",
    "    return base64_str, png_bytes, resized_img\n",
    "\n",
    "# Callback: QC Image Upload\n",
    "def on_qc_upload(change):\n",
    "    if qc_uploader.value:\n",
    "        file = qc_uploader.value[0]\n",
    "        b64, raw, preview = get_png_base64_from_bytes(file['content'])\n",
    "        global base64_image, image_data, img\n",
    "        base64_image, image_data, img = b64, raw, preview\n",
    "        with qc_display:\n",
    "            clear_output(wait=True)\n",
    "            display(img)\n",
    "\n",
    "# Callback: Reference Image Upload\n",
    "def on_ref_upload(change):\n",
    "    if ref_uploader.value:\n",
    "        file = ref_uploader.value[0]\n",
    "        b64, raw, preview = get_png_base64_from_bytes(file['content'])\n",
    "        global base64_reference_image, reference_image_data, ref_img\n",
    "        base64_reference_image, reference_image_data, ref_img = b64, raw, preview\n",
    "        with ref_display:\n",
    "            clear_output(wait=True)\n",
    "            display(ref_img)\n",
    "\n",
    "# Attach callbacks\n",
    "qc_uploader.observe(on_qc_upload, names='value')\n",
    "ref_uploader.observe(on_ref_upload, names='value')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Define the system prompt. As you see it defines the special awareness so we can calculate bounding<br/>boxes to the images after the model has run.\n",
    "\n",
    "You also see that we request a specific JSON structure to automatically process the output of the model.\n",
    "\n",
    "We then add the images as base64 encoded images to the prompt and send the created prompt to Bedrock.\n",
    "\n",
    "The response from Bedrock is cleaned in case that there is markdown included so we can automatically process the response."
   ],
   "id": "d5dbe138f6a96133"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "86205876-d2b5-4808-89c7-585490f02ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference completed successfully.\n",
      "✅ Parsed QC Output:\n",
      "{\n",
      "  \"text\": \"The image shows a box of cigars. Each cigar is wrapped in a black and gold wrapper with a logo. The cigars are neatly arranged in rows. There is a yellow piece of tape on one of the cigars, which is not standard and indicates a potential issue with that particular cigar.\",\n",
      "  \"objects\": [\n",
      "    {\n",
      "      \"name\": \"cigar\",\n",
      "      \"color\": \"brown\",\n",
      "      \"qc\": \"NOK\",\n",
      "      \"reason\": \"yellow tape\",\n",
      "      \"bounding_box\": {\n",
      "        \"x_min\": 290,\n",
      "        \"y_min\": 500,\n",
      "        \"x_max\": 380,\n",
      "        \"y_max\": 680\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# System prompt\n",
    "default_system_prompt = dedent(\"\"\"\\\n",
    "    You are an object detector. When the user provides you with an image, provide a JSON with the following content:\n",
    "    1. Describe the color and features of the objects on the image.\n",
    "    Give an explanation of the object from a quality control perspective, from a manufacturing point of view.\n",
    "    That object is a cube, can be blue, yellow, green, or red. Give the color and the quality analysis in a simple and quick way.\n",
    "    The color must be consistent. If you see that it has other colors, or any other object obstructing or on top of it, consider it NOK.\n",
    "    Check irregular edges too.\n",
    "    Provide coordinates in pixels for bounding boxes to detect the defects.\n",
    "    Always measure based on X being the wider part of the image.\n",
    "    Always use x_min, y_min, x_max, y_max.\n",
    "    Give me only the coordinates of the defects.\n",
    "    When multiple defects appear, give me the bounding boxes of all of them.\n",
    "    Do not group them. Do not create bounding boxes from the reference image.\n",
    "    When the image is not clear or unusual, consider it NOK.\n",
    "    Differentiate between color and obstruction. Check any image differences.\n",
    "    Limit the answer to:\n",
    "        QC: OK — if it looks fine\n",
    "        QC: NOK — if it has defects (with a short reason)\n",
    "    Use the tag \"text\".\n",
    "\"\"\")\n",
    "\n",
    "# Prompt that requests a structured JSON\n",
    "instruction = dedent(\"\"\"\\\n",
    "    Provide me the JSON with the written description in 'text' and the list of objects in 'objects'.\n",
    "    This list must include: name, color, qc, reason, bounding_box of the defect (x_min, y_min, x_max, y_max).\n",
    "    Clean JSON only — no markdown, no extra characters.\n",
    "    If you describe a defect, include its bounding box.\n",
    "    Do not group bounding boxes or include any from the reference image.\n",
    "    Example JSON:\n",
    "    {\n",
    "        \"text\": \"The object is a blue and green sponge. The green part has white spots...\",\n",
    "        \"objects\": [\n",
    "            {\n",
    "                \"name\": \"defect\",\n",
    "                \"color\": \"green\",\n",
    "                \"qc\": \"NOK\",\n",
    "                \"reason\": \"white spots\",\n",
    "                \"bounding_box\": {\n",
    "                    \"x_min\": 195, \"y_min\": 475, \"x_max\": 285, \"y_max\": 595\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"defect\",\n",
    "                \"color\": \"blue\",\n",
    "                \"qc\": \"NOK\",\n",
    "                \"reason\": \"white spot and small hole\",\n",
    "                \"bounding_box\": {\n",
    "                    \"x_min\": 690, \"y_min\": 420, \"x_max\": 760, \"y_max\": 500\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "# Construct the request\n",
    "system_list = [{\"text\": default_system_prompt}]\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"image\": {\"format\": \"png\", \"source\": {\"bytes\": base64.b64encode(image_data).decode()}}},\n",
    "            {\"text\": instruction}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "if reference_image_data:\n",
    "    message_list.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": \"This is the reference image. Do not include bounding boxes for this image.\"},\n",
    "            {\"image\": {\"format\": \"png\", \"source\": {\"bytes\": base64.b64encode(reference_image_data).decode()}}}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "native_request = {\n",
    "    \"schemaVersion\": \"messages-v1\",\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": {\n",
    "        \"max_new_tokens\": 2500,\n",
    "        \"top_p\": 0.1,\n",
    "        \"top_k\": 20,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = bedrock_runtime.invoke_model(modelId=MODEL_ID, body=json.dumps(native_request))\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "    print(\"✅ Inference completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Model invocation error: {str(e)}\")\n",
    "\n",
    "raw_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "try:\n",
    "    # Remove markdown or formatting just in case\n",
    "    clean_json = raw_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    parsed = json.loads(clean_json)\n",
    "    print(\"✅ Parsed QC Output:\")\n",
    "    print(json.dumps(parsed, indent=2))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"❌ JSON parsing failed:\", str(e))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have the response we need to draw the bounding boxes of the detected faults over the images.\n",
    "Matplotlib helps to do this.\n",
    "\n",
    "Afterwards we show the reference image, the resuling image with drawing bouning boxes if there were detected errors <br/>\n",
    "and the result description given by the model."
   ],
   "id": "68c6c28415b63b71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def draw_bounding_boxes(base64_img, objects):\n",
    "    img = Image.open(io.BytesIO(base64.b64decode(base64_img)))\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    # Get the resolution from the first object\n",
    "    resolution = objects[0].get(\"image_resolution\", \"1000x1000\")\n",
    "    model_width, model_height = map(int, resolution.lower().split(\"x\"))\n",
    "\n",
    "    scale_x = img_width / model_width\n",
    "    scale_y = img_height / model_height\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img)\n",
    "\n",
    "    for obj in objects:\n",
    "        box = obj[\"bounding_box\"]\n",
    "        x_min = box[\"x_min\"] * scale_x\n",
    "        y_min = box[\"y_min\"] * scale_y\n",
    "        x_max = box[\"x_max\"] * scale_x\n",
    "        y_max = box[\"y_max\"] * scale_y\n",
    "        label = f\"{obj['name']} ({obj['qc']})\"\n",
    "        color = \"green\" if obj[\"qc\"] == \"OK\" else \"red\"\n",
    "\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                             linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min - 10, label, color=color, fontsize=12, weight='bold')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "if base64_reference_image:\n",
    "    ref_img = Image.open(io.BytesIO(base64.b64decode(base64_reference_image)))\n",
    "    w, h = ref_img.size\n",
    "    resized = ref_img.resize((w // 2, h // 2))\n",
    "    print(\"🧾 Reference Image:\")\n",
    "    display(resized)\n",
    "\n",
    "print(\"🧾 Quality Control Report:\")\n",
    "if parsed.get(\"objects\"):\n",
    "    report_text = f\"\"\"QC: {parsed['objects'][0]['qc']}\n",
    "\n",
    "Description:\n",
    "{parsed['text']}\n",
    "\n",
    "Defects:\"\"\"\n",
    "\n",
    "    for obj in parsed[\"objects\"]:\n",
    "        box = obj[\"bounding_box\"]\n",
    "        report_text += f\"\"\"\n",
    "- {obj['name']} ({obj['color']}): {obj['qc']} — reason: {obj['reason']}\n",
    "  Bounding Box: x_min={box['x_min']}, y_min={box['y_min']}, x_max={box['x_max']}, y_max={box['y_max']}\n",
    "\"\"\"\n",
    "    draw_bounding_boxes(base64_image, parsed[\"objects\"])\n",
    "else:\n",
    "    report_text = f\"\"\"QC: OK\n",
    "\n",
    "Description:\n",
    "{parsed['text']}\n",
    "\n",
    "No defects were detected.\n",
    "\"\"\"\n",
    "    if base64_image:\n",
    "        qc_img = Image.open(io.BytesIO(base64.b64decode(base64_image)))\n",
    "        w, h = qc_img.size\n",
    "        resized = qc_img.resize((w // 2, h // 2))\n",
    "        print(\"🧾 QC Image:\")\n",
    "        display(resized)\n",
    "\n",
    "print(report_text)"
   ],
   "id": "3a4e0dd1d8031f61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With this simple steps you see how the whole pipeline works.",
   "id": "7893dcc80062d391"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
